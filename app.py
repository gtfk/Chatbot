# Versi贸n 1.8 - Intentando nuevamente con las 煤ltimas versiones y rutas de importaci贸n actualizadas
import streamlit as st
from langchain_groq import ChatGroq
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_community.retrievers import BM25Retriever
# --- Importaci贸n Espec铆fica para EnsembleRetriever (ltimas Versiones) ---
from langchain.retrievers.ensemble import EnsembleRetriever
# --- Fin ---
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate
import os
import langchain # Para verificar la versi贸n

# --- CONFIGURACIN DE LA PGINA ---
st.set_page_config(page_title="Chatbot Acad茅mico Duoc UC", page_icon="", layout="wide")
st.title(" Chatbot del Reglamento Acad茅mico")

# --- CARGA DE LA API KEY DE GROQ ---
GROQ_API_KEY = st.secrets.get("GROQ_API_KEY")

if not GROQ_API_KEY:
    st.error("La clave de API de Groq no est谩 configurada. Por favor, agr茅gala a los Secrets de Streamlit.")
    st.stop()

# --- CACHING DE RECURSOS ---
# A帽adimos allow_output_mutation=True por si acaso con objetos complejos
@st.cache_resource(allow_output_mutation=True)
def inicializar_cadena():
    # L铆nea de depuraci贸n para la versi贸n
    st.write(f"Inicializando con LangChain v{langchain.__version__}")

    # --- 1. Cargar y Procesar el PDF ---
    loader = PyPDFLoader("reglamento.pdf")
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)
    docs = loader.load_and_split(text_splitter=text_splitter)
    st.write(f"PDF procesado en {len(docs)} fragmentos.") # Depuraci贸n

    # --- 2. Crear los Embeddings y el Ensemble Retriever ---
    embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
    vector_store = Chroma.from_documents(docs, embeddings)
    vector_retriever = vector_store.as_retriever(search_kwargs={"k": 7})
    doc_texts = [doc.page_content for doc in docs] # BM25 necesita textos
    bm25_retriever = BM25Retriever.from_texts(doc_texts)
    bm25_retriever.k = 7
    retriever = EnsembleRetriever(retrievers=[bm25_retriever, vector_retriever], weights=[0.7, 0.3])
    st.write("Retrievers creados.") # Depuraci贸n

    # --- 3. Conectarse al Modelo en Groq Cloud ---
    llm = ChatGroq(
        api_key=GROQ_API_KEY,
        model="llama-3.1-8b-instant",
        temperature=0.1
    )
    st.write("Conexi贸n con Groq establecida.") # Depuraci贸n

    # --- 4. Crear la Cadena de Conversaci贸n ---
    prompt = ChatPromptTemplate.from_template("""
    INSTRUCCIN PRINCIPAL: Responde SIEMPRE en espa帽ol.
    Eres un asistente experto en el reglamento acad茅mico de Duoc UC. Tu objetivo es dar respuestas claras y precisas basadas NICAMENTE en el contexto proporcionado.
    Si la pregunta es general sobre "qu茅 debe saber un alumno nuevo", crea un resumen que cubra los puntos clave: Asistencia, Calificaciones para aprobar, y Causas de Reprobaci贸n.

    CONTEXTO:
    {context}

    PREGUNTA:
    {input}

    RESPUESTA:
    """)
    document_chain = create_stuff_documents_chain(llm, prompt)
    retrieval_chain = create_retrieval_chain(retriever, document_chain)
    st.write("Cadena de LangChain lista.") # Depuraci贸n

    return retrieval_chain

# --- LGICA DE LA APLICACIN DE CHAT ---
try:
    retrieval_chain = inicializar_cadena()

    if "messages" not in st.session_state:
        st.session_state.messages = []

    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    if prompt := st.chat_input("驴Qu茅 duda tienes sobre el reglamento?"):
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        with st.chat_message("assistant"):
            with st.spinner("Pensando... "):
                response = retrieval_chain.invoke({"input": prompt})
                st.markdown(response["answer"])

        st.session_state.messages.append({"role": "assistant", "content": response["answer"]})

except Exception as e:
    st.error(f"Ha ocurrido un error durante la ejecuci贸n: {e}")
    # A帽adimos m谩s detalle al error si es posible
    st.exception(e)